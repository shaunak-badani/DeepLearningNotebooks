{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Model-Created Embeddings in PyTorch\n",
    "In this notebook we will be doing text classification by using PyTorch embeddings to represent for each document in the dataset, and we will learn the embeddings from scratch using our training data, rather than using pre-trained embeddings. The embeddings will then be used to represent our documents as features and input into a classification model.  Our goal will be to classify the articles in the AgNews dataset into their correct category: \"World\", \"Sports\", \"Business\", or \"Sci/Tec\".\n",
    "\n",
    "To create the embedding for each document, we will first create embeddings for each word in the document.  We will then use the mean embedding for all words in a document as the embedding to represent the document.  The document embedding will then serve as the feature set to feed into a single-layer linear classifier which performs softmax regression with cross entropy loss to classify the documents.\n",
    "\n",
    "**Notes:**  \n",
    "- This does not need to run on GPU, but will take ~5 minutes to run on CPU\n",
    "\n",
    "**References:**  \n",
    "- This notebook includes portions of code from the [PyTorch docs tutorials](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import copy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch import nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4            3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3  Reuters - Authorities have halted oil export\\f...   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                           full_text  \n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...  \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "4  Oil prices soar to all-time record, posing new...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the data\n",
    "if not os.path.exists('../data'):\n",
    "    os.mkdir('../data')\n",
    "if not os.path.exists('../data/agnews'):\n",
    "    url = 'https://storage.googleapis.com/aipi540-datasets/agnews.zip'\n",
    "    urllib.request.urlretrieve(url,filename='../data/agnews.zip')\n",
    "    zip_ref = zipfile.ZipFile('../data/agnews.zip', 'r')\n",
    "    zip_ref.extractall('../data/agnews')\n",
    "    zip_ref.close()\n",
    "\n",
    "train_df = pd.read_csv('../data/agnews/train.csv')\n",
    "test_df = pd.read_csv('../data/agnews/test.csv')\n",
    "\n",
    "# Combine title and description of article to use as input documents for model\n",
    "train_df['full_text'] = train_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
    "test_df['full_text'] = test_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
    "\n",
    "# Create dictionary to store mapping of labels\n",
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "\n",
      "Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "\n",
      "Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "\n",
      "Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View a couple of the documents\n",
    "for i in range(5):\n",
    "    print(train_df.iloc[i]['full_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Datasets\n",
    "Now that our data is loaded, we first need to prepare our data by putting it into PyTorch Dataset format.  We will also split our training data to create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data in iterator form needed to create PyTorch Datasets from data\n",
    "train_iter = [(label,text) for label,text in zip(train_df['Class Index'].to_list(),train_df['full_text'].to_list())]\n",
    "test_iter = [(label,text) for label,text in zip(test_df['Class Index'].to_list(),test_df['full_text'].to_list())]\n",
    "\n",
    "# Create PyTorch Datasets from iterators\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Split training data to get a validation set\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_dataset, split_valid_dataset = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Data in DataLoaders\n",
    "We are now ready to create PyTorch DataLoaders from our Datasets, which we can use to feed mini-batches of inputs and labels to our model.  \n",
    "\n",
    "However, we want to perform a couple operations on the data which is loaded into each mini-batch.  We can define a custom `collate_fn()` to perform these operations, which then is applied to the data loaded into each batch in the DataLoader.  We want to accomplish the following in our `collate_fn()`:  \n",
    "- Tokenize the text data to form a list of tokens for each document  \n",
    "- Convert the token list for each document into a list of integers.  We do this by creating the \"vocabulary\" out of all tokens found in the training data as an array, and then represent each document as a list of integers representing the index positions of the document's words in the vocabulary\n",
    "- Store the locations of the \"offsets\" or delimiter positions between each document in the minibatch.  Since the samples in the batch are concatenated into a single tensor for the input to the `nn.EmbeddingBag` layer in our model, we need to store the delimiter positions representing the beginning index of each individual document sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the text\n",
    "def yield_tokens(data_iter,tokenizer):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build vocabulary from tokens of training set\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter,tokenizer), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Define collate_batch function to get single collated tensor for batch in form needed by nn.EmbeddingBag\n",
    "def collate_batch(batch,tokenizer,vocab):\n",
    "    # Pipelines for processing text and labels\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    label_pipeline = lambda x: int(x) - 1\n",
    "    \n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    # Iterate through batch, processing text and adding text, labels and offsets to lists\n",
    "    for (label, text) in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Create training, validation and test set DataLoaders using custom collate_batch function\n",
    "train_dataloader = DataLoader(split_train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn = lambda x: collate_batch(x,tokenizer,vocab))\n",
    "val_dataloader = DataLoader(split_valid_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn = lambda x: collate_batch(x,tokenizer,vocab))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn = lambda x: collate_batch(x,tokenizer,vocab))\n",
    "\n",
    "# Set up dict for dataloaders to use in training\n",
    "train_dataloaders = {'train':train_dataloader,'val':val_dataloader}\n",
    "\n",
    "# Store size of training and validation sets\n",
    "dataset_sizes = {'train':len(split_train_dataset),'val':len(split_valid_dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Now that we have our data in DataLoaders, we are ready to train our classification model.  Our model will be composed of two layers:  \n",
    "1) A [nn.EmbeddingBag](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag) layer which converts each word / n-gram into an embedding and then takes the mean or sum of the embeddings of all words/n-grams in a document as the embedding vector representing the document.  We can specify the size of the embedding vector we wish to create to represent each document\n",
    "2) A fully connected nn.Linear layer which takes the document embedding as input and then attempts to classify the document based on the embedding.\n",
    "\n",
    "![](.img/text_model_pytorch.png)\n",
    "\n",
    "*Figure from the [PyTorch docs](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\",sparse=True)\n",
    "        # Fully connected final layer to convert embeddings to output predictions\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up a function to train our model.  Our `train_model()` function below will train our model and report out the training set and validation set performance at each epoch.  The function will store the model weights corresponding to the weights which achieved the best validation set performance during the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, scheduler, device, num_epochs=5):\n",
    "    model = model.to(device) # Send model to GPU if available\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Get the input images and labels, and send to GPU if available\n",
    "            for (labels, text, offsets) in dataloaders[phase]:\n",
    "                text = text.to(device)\n",
    "                labels = labels.to(device)\n",
    "                offsets = offsets.to(device)\n",
    "\n",
    "                # Zero the weight gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass to get outputs and calculate loss\n",
    "                # Track gradient only for training data\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model.forward(text,offsets)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backpropagation to get the gradients with respect to each weight\n",
    "                    # Only if in train\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        # Update the weights\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Convert loss into a scalar and add it to running_loss\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                # Track number of correct predictions\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Step along learning rate scheduler when in train\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            # Calculate and display average loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print('{} loss: {:.4f} accuracy: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # If model performs better on val set, save weights as the best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best validation set accuracy: {:3f}'.format(best_acc))\n",
    "\n",
    "    # Load the weights from best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train loss: 0.7667 accuracy: 0.7195\n",
      "val loss: 0.4612 accuracy: 0.8528\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train loss: 0.3923 accuracy: 0.8711\n",
      "val loss: 0.3817 accuracy: 0.8770\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train loss: 0.3354 accuracy: 0.8908\n",
      "val loss: 0.3553 accuracy: 0.8887\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train loss: 0.3086 accuracy: 0.9001\n",
      "val loss: 0.3380 accuracy: 0.8928\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train loss: 0.2923 accuracy: 0.9053\n",
      "val loss: 0.3312 accuracy: 0.8918\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train loss: 0.2813 accuracy: 0.9086\n",
      "val loss: 0.3245 accuracy: 0.8953\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train loss: 0.2732 accuracy: 0.9116\n",
      "val loss: 0.3197 accuracy: 0.9000\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train loss: 0.2672 accuracy: 0.9131\n",
      "val loss: 0.3178 accuracy: 0.8995\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train loss: 0.2627 accuracy: 0.9144\n",
      "val loss: 0.3170 accuracy: 0.9020\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train loss: 0.2591 accuracy: 0.9157\n",
      "val loss: 0.3134 accuracy: 0.9025\n",
      "\n",
      "Training complete in 1m 35s\n",
      "Best validation set accuracy: 0.902500\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "num_classes = len(set([label for (label, _) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64 # Set desired document embedding size\n",
    "nn_model = TextClassificationModel(vocab_size, embed_dim, num_classes)\n",
    "\n",
    "# Set hyperparameters\n",
    "epochs = 10 # epoch\n",
    "learning_rate = 1.  # learning rate\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1 , gamma=0.8)\n",
    "\n",
    "# Train the model\n",
    "nn_model = train_model(nn_model, criterion, optimizer, train_dataloaders, lr_scheduler, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "Now that we have trained our model, we can evaluate its performance using our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model):\n",
    "    # Generate predictions and calculate accuracy\n",
    "    nn_model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model.forward(text, offsets)\n",
    "            #loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy    0.895\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on the test dataset\n",
    "accu_test = evaluate(test_dataloader, nn_model)\n",
    "print('test set accuracy {:8.3f}'.format(accu_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
