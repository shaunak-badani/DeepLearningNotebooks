{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Word Counts / TFIDF\n",
    "In this notebook we will be performing text classification by using word counts and frequency to create numerical feature vectors representing each text document and then using these features to train a simple classifier.  Although simple, we will see that this approach can work very well for classifying text, even compared to more modern document embedding approaches.  Our goal will be to classify the articles in the AgNews dataset into their correct category: \"World\", \"Sports\", \"Business\", or \"Sci/Tec\".\n",
    "\n",
    "**Notes:**  \n",
    "- This does not need to be run on GPU, but will take ~5 minutes to run  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/jjr10/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "#!python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4            3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3  Reuters - Authorities have halted oil export\\f...   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                           full_text  \n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...  \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "4  Oil prices soar to all-time record, posing new...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the data\n",
    "if not os.path.exists('../data'):\n",
    "    os.mkdir('../data')\n",
    "if not os.path.exists('../data/agnews'):\n",
    "    url = 'https://storage.googleapis.com/aipi540-datasets/agnews.zip'\n",
    "    urllib.request.urlretrieve(url,filename='../data/agnews.zip')\n",
    "    zip_ref = zipfile.ZipFile('../data/agnews.zip', 'r')\n",
    "    zip_ref.extractall('../data/agnews')\n",
    "    zip_ref.close()\n",
    "\n",
    "train_df = pd.read_csv('../data/agnews/train.csv')\n",
    "test_df = pd.read_csv('../data/agnews/test.csv')\n",
    "\n",
    "# Combine title and description of article to use as input documents for model\n",
    "train_df['full_text'] = train_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
    "test_df['full_text'] = test_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
    "\n",
    "# Create dictionary to store mapping of labels\n",
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "\n",
      "Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "\n",
      "Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "\n",
      "Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View a couple of the documents\n",
    "for i in range(5):\n",
    "    print(train_df.iloc[i]['full_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text\n",
    "Before we create our features, we first need to pre-process our text.  There are several methods to pre-process text; in this example we will perform the following operations on our raw text to prepare it for creating features:  \n",
    "- Tokenize our raw text to break it into a list of substrings.  This step primarily splits our text on white space and punctuation.  As an example from the [NLTK](https://www.nltk.org/api/nltk.tokenize.html) website:  \n",
    "\n",
    "    ```\n",
    "    >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.\"\n",
    "    >>> word_tokenize(s)\n",
    "    ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',\n",
    "    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
    "    ```\n",
    "\n",
    "- Remove punctuation and stopwords.  Stopwords are extremely commonly used words (e.g. \"a\", \"and\", \"are\", \"be\", \"from\" ...) that do not provide any useful information to us to assist in modeling the text.  \n",
    "\n",
    "- Lemmatize the words in each document.  Lemmatization uses a morphological analysis of words to remove inflectional endings and return the base or dictionary form of words, called the \"lemma\".  Among other things, this helps by replacing plurals with singular form e.g. \"dogs\" becomes \"dog\" and \"geese\" becomes \"goose\".  This is particularly important when we are using word counts or freqency because we want to count the occurences of \"dog\" and \"dogs\" as the same word.  \n",
    "\n",
    "There are several libraries available in Python to process text.  Below we have shown how to perform the above operations using two of the most popular: [NLTK](https://www.nltk.org) and [Spacy](https://spacy.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,method='spacy'):\n",
    "# Tokenize and lemmatize text, remove stopwords and punctuation\n",
    "\n",
    "    punctuations = string.punctuation\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    if method=='nltk':\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(sentence,preserve_line=True)\n",
    "        # Remove stopwords and punctuation\n",
    "        tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "        # Lemmatize\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "        tokens = \" \".join([i for i in tokens])\n",
    "    else:\n",
    "        # Tokenize\n",
    "        with nlp.select_pipes(enable=['tokenizer','lemmatizer']):\n",
    "            tokens = nlp(sentence)\n",
    "        # Lemmatize\n",
    "        tokens = [word.lemma_.lower().strip() for word in tokens]\n",
    "        # Remove stopwords and punctuation\n",
    "        tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "        tokens = \" \".join([i for i in tokens])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:48<00:00, 2498.64it/s]\n",
      "100%|██████████| 7600/7600 [00:03<00:00, 2500.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the training set text\n",
    "tqdm.pandas()\n",
    "train_df['processed_text'] = train_df['full_text'].progress_apply(lambda x: tokenize(x,method='nltk'))\n",
    "\n",
    "# Process the test set text\n",
    "tqdm.pandas()\n",
    "test_df['processed_text'] = test_df['full_text'].progress_apply(lambda x: tokenize(x,method='nltk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features using word counts\n",
    "Now that our raw text is pre-processed, we are ready to create our features.  There are two approaches to creating features using word counts: **Count Vectorization** and **TFIDF Vectorization**.\n",
    "\n",
    "**Count Vectorization** (also called Bag-of-words) creates a vocabulary of all words appearing in the training corpus, and then for each document it counts up how many times each word in the vocabulary appears in the document.  Each document is then represented by a vector with the same length as the vocabulary.  At each index position an integer indicates how many times each word appears in the document.\n",
    "\n",
    "**Term Frequency Inverse Document Frequency (TFIDF) Vectorization** first counts the number of times each word appears in a document (similar to Count Vectorization) but then divides by the total number of words in the document to calculate the *term frequency (TF)* of each word.  The *inverse document frequency (IDF)* for each word is then calculated as the log of the total number of documents divided by the number of documents containing the word.  The TFIDF for each word is then computed by multiplying the term frequency by the inverse document frequency.  Each document is represented by a vector containing the TFIDF for every word in the vocabulary, for that document.\n",
    "\n",
    "In the below `build_features()` function, you can specify whether to create document features using Count Vectorization or TFIDF Vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(train_data, test_data, ngram_range, method='count'):\n",
    "    if method == 'tfidf':\n",
    "        # Create features using TFIDF\n",
    "        vec = TfidfVectorizer(ngram_range=ngram_range)\n",
    "        X_train = vec.fit_transform(train_df['processed_text'])\n",
    "        X_test = vec.transform(test_df['processed_text'])\n",
    "\n",
    "    else:\n",
    "        # Create features using word counts\n",
    "        vec = CountVectorizer(ngram_range=ngram_range)\n",
    "        X_train = vec.fit_transform(train_df['processed_text'])\n",
    "        X_test = vec.transform(test_df['processed_text'])\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "method = 'tfidf'\n",
    "ngram_range = (1, 2)\n",
    "X_train,X_test = build_features(train_df['processed_text'],test_df['processed_text'],ngram_range,method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Now that we have created our features representing each document, we will use them in a simple softmax regression classification model to predict the document's class.  We first train the classification model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set is 0.961\n"
     ]
    }
   ],
   "source": [
    "# Train a classification model using logistic regression classifier\n",
    "y_train = train_df['Class Index']\n",
    "logreg_model = LogisticRegression(solver='saga')\n",
    "logreg_model.fit(X_train,y_train)\n",
    "preds = logreg_model.predict(X_train)\n",
    "acc = sum(preds==y_train)/len(y_train)\n",
    "print('Accuracy on the training set is {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "We then evaluate our model on the test set.  As you can see, the model performs very well on this task, using this simple approach!  In general, Count Vectorization / TFIDF Vectorization performs surprising well across a broad range of tasks, even compared to more computationally intensive approaches such as document embeddings.  This should perhaps not be surprising, since we would expect documents about similar topics to contain similar sets of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is 0.919\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on the test set\n",
    "y_test = test_df['Class Index']\n",
    "test_preds = logreg_model.predict(X_test)\n",
    "test_acc = sum(test_preds==y_test)/len(y_test)\n",
    "print('Accuracy on the test set is {:.3f}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
